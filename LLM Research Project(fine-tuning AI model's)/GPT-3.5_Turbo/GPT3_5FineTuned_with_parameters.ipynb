{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eD4rrQtIOIU",
        "outputId": "6d07df43-9bd6-4a2e-b5e3-0f0ca3594421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv(\"PromptDataset.csv\") # paste the path in these quotes\n",
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SQhCTL1aPyIu",
        "outputId": "8d1c1d76-5b24-4475-8d94-f7355d6a681f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Prompt  \\\n",
              "0                     What is the capital of France?   \n",
              "1                          Explain the water cycle.    \n",
              "2  Name the three branches of the U.S. government...   \n",
              "3                 Describe how photosynthesis works.   \n",
              "4  What are the main differences between classica...   \n",
              "\n",
              "                                            Response  \n",
              "0                    The capital of France is Paris.  \n",
              "1  The water cycle is the continuous movement of ...  \n",
              "2  The three branches of the U.S. government are ...  \n",
              "3  Photosynthesis is the process in which green p...  \n",
              "4  The main differences between classical and qua...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf8d07d3-7344-406f-b81b-6d96461f79fa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Prompt</th>\n",
              "      <th>Response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the capital of France?</td>\n",
              "      <td>The capital of France is Paris.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Explain the water cycle.</td>\n",
              "      <td>The water cycle is the continuous movement of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Name the three branches of the U.S. government...</td>\n",
              "      <td>The three branches of the U.S. government are ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Describe how photosynthesis works.</td>\n",
              "      <td>Photosynthesis is the process in which green p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the main differences between classica...</td>\n",
              "      <td>The main differences between classical and qua...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf8d07d3-7344-406f-b81b-6d96461f79fa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bf8d07d3-7344-406f-b81b-6d96461f79fa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bf8d07d3-7344-406f-b81b-6d96461f79fa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9a203823-6788-4802-bc3d-0007e9c721c7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9a203823-6788-4802-bc3d-0007e9c721c7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9a203823-6788-4802-bc3d-0007e9c721c7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"Prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Describe the climate and geography of the Sahara Desert.\",\n          \"What are the ethical considerations in animal testing for medical research?\",\n          \"Discuss the concpt of free will versus determinism.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"The Sahara Desert has an exceptionally dry environment, with daytime highs of over 100 degrees Celsius and nighttime lows of around 60 degrees. It covers most of North Africa and is typified by long areas of dunes, rocky plateaus, gravel plains, and scattered mountains and lakes.\\r\",\n          \"Animal testing in medical research raises ethical concerns regarding animal welfare, consent, and necessity. The 3Rs principle\\u2014Replacement, Reduction, and Refinement\\u2014guides ethical research by advocating for alternatives to animal use, minimizing the number of animals used, and refining procedures to reduce suffering. Ethical considerations also involve balancing scientific advancement with humane treatment, ensuring that animal testing is justified and that benefits outweigh ethical costs.\\r\",\n          \"The debate between free will and determinism centers on whether humans have the ability to make choices independent of external forces or if every action is predetermined by factors such as biology and environment. Proponents of free will argue that individuals can make conscious decisions, reflecting personal autonomy. Determinists, however, claim that all events, including human actions, are the result of preceding causes. Compatibilists attempt to reconcile the two by suggesting that free will is compatible with determinism, arguing that people can still be responsible for their actions even if they are determined by prior states.\\r\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "9LMeaoooIlCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgUdFMQUP9CI",
        "outputId": "9e65c5a4-b5fc-4eae-cb6a-fbd1002e93fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.40.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.40.3-py3-none-any.whl (360 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.7/360.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 openai-1.40.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 80% training, 10% validation, and 10% test\n",
        "train_size = 0.8\n",
        "# we will use the val for adjusting the hyperparameters\n",
        "val_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "train_df, temp_df = train_test_split(dataset, train_size=train_size, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=test_size / (val_size + test_size), random_state=42)\n",
        "\n",
        "train_df.to_csv('PromptDataset_train.csv', index=False)\n",
        "val_df.to_csv('PromptDataset_val.csv', index=False)\n",
        "test_df.to_csv('PromptDataset_test.csv', index=False)\n",
        "\n",
        "print(\"Train/validation/test split completed. Files saved as 'PromptDataset_train.csv', 'PromptDataset_val.csv', and 'PromptDataset_test.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OybuPZyk6Rza",
        "outputId": "c219bded-9d72-46dd-8a7b-ce9cb568a341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/validation/test split completed. Files saved as 'PromptDataset_train.csv', 'PromptDataset_val.csv', and 'PromptDataset_test.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the validation DataFrame\n",
        "val_df = pd.read_csv('PromptDataset_val.csv')\n",
        "\n",
        "# Print column names to verify\n",
        "print(\"Columns in the DataFrame:\", val_df.columns)\n",
        "\n",
        "# Check the first few rows to confirm correct data loading\n",
        "print(val_df.head())\n",
        "reference_answer = row\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "SaAw-wXpn8np",
        "outputId": "7177b039-ad0a-48ba-96c6-08641f2aba77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the DataFrame: Index(['Prompt', 'Response'], dtype='object')\n",
            "                                              Prompt  \\\n",
            "0  Discuss the concpt of free will versus determi...   \n",
            "1  Discuss the impact of social media on mental h...   \n",
            "2  Describe the cultural impact of the Beatles on...   \n",
            "3  How can you prioritize tasks when you have mul...   \n",
            "4  How do internships benefit students in their c...   \n",
            "\n",
            "                                            Response  \n",
            "0  The debate between free will and determinism c...  \n",
            "1  Social media has a huge impact on mental healt...  \n",
            "2  The Beatles had a profound cultural impact on ...  \n",
            "3  I prioritize tasks when I have multiple deadli...  \n",
            "4  Internships benefit students in their career d...  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'row' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-14f449e11b2c>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Check the first few rows to confirm correct data loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mreference_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'row' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and split dataset into validation and test sets\n",
        "def split_dataset(dataset, validation_size=0.5, test_size=0.5):\n",
        "    assert (validation_size + test_size) == 1.0, \"Sizes must sum to 1\"\n",
        "    validation_data, test_data = train_test_split(dataset, test_size=test_size)\n",
        "    return validation_data, test_data\n",
        "\n",
        "# Define your dataset\n",
        "dataset = pd.read_csv(\"PromptDataset.csv\")\n",
        "\n",
        "# Split dataset\n",
        "validation_data, test_data = split_dataset(dataset)\n",
        "\n",
        "# Define hyperparameter values to test\n",
        "n_epochs_values = [10, 20, 30]\n",
        "batch_size_values = [4, 8, 16]\n",
        "learning_rate_values = [0.1, 0.3, 0.5]\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key= 'key\n",
        "openai.api_key = 'key'\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating text: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to evaluate hyperparameters using cosine similarity on validation data\n",
        "def evaluate_hyperparameters(n_epochs, batch_size, learning_rate, val_df):\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        reference_answers = val_df['Response'].tolist()\n",
        "        vectorizer.fit(reference_answers)\n",
        "\n",
        "        max_score = -np.inf  # Initialize to negative infinity\n",
        "\n",
        "        for index, row in val_df.iterrows():\n",
        "            prompt = row['Prompt']\n",
        "            reference_answer = row['Response']\n",
        "            generated_text = generate_text(prompt)\n",
        "\n",
        "            # Vectorize the generated text and the reference answer\n",
        "            generated_vector = vectorizer.transform([generated_text])\n",
        "            reference_vector = vectorizer.transform([reference_answer])\n",
        "\n",
        "            # Compute cosine similarity between the generated text and reference answer\n",
        "            cosine_similarity_score = cosine_similarity(generated_vector, reference_vector)[0][0]\n",
        "\n",
        "            # Track the highest similarity score\n",
        "            if cosine_similarity_score > max_score:\n",
        "                max_score = cosine_similarity_score\n",
        "\n",
        "        return max_score\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in evaluation: {e}\")\n",
        "        return 0\n",
        "\n",
        "# Grid search for hyperparameters using validation data only\n",
        "best_hyperparams = None\n",
        "best_score = float('-inf')\n",
        "\n",
        "for n_epochs, batch_size, learning_rate in itertools.product(n_epochs_values, batch_size_values, learning_rate_values):\n",
        "    print(f\"Testing: n_epochs={n_epochs}, batch_size={batch_size}, learning_rate={learning_rate}\")\n",
        "\n",
        "    score = evaluate_hyperparameters(n_epochs, batch_size, learning_rate, validation_data)\n",
        "\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_hyperparams = {\n",
        "            \"n_epochs\": n_epochs,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"learning_rate_multiplier\": learning_rate\n",
        "        }\n",
        "\n",
        "print(f\"Best hyperparameters: {best_hyperparams}\")\n",
        "print(f\"Best validation score: {best_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvlkn6l0okgE",
        "outputId": "e212accd-5ba3-43f0-d93e-1bbeae25cd25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: n_epochs=10, batch_size=4, learning_rate=0.1\n",
            "Testing: n_epochs=10, batch_size=4, learning_rate=0.3\n",
            "Testing: n_epochs=10, batch_size=4, learning_rate=0.5\n",
            "Testing: n_epochs=10, batch_size=8, learning_rate=0.1\n",
            "Testing: n_epochs=10, batch_size=8, learning_rate=0.3\n",
            "Testing: n_epochs=10, batch_size=8, learning_rate=0.5\n",
            "Testing: n_epochs=10, batch_size=16, learning_rate=0.1\n",
            "Testing: n_epochs=10, batch_size=16, learning_rate=0.3\n",
            "Testing: n_epochs=10, batch_size=16, learning_rate=0.5\n",
            "Testing: n_epochs=20, batch_size=4, learning_rate=0.1\n",
            "Testing: n_epochs=20, batch_size=4, learning_rate=0.3\n",
            "Testing: n_epochs=20, batch_size=4, learning_rate=0.5\n",
            "Testing: n_epochs=20, batch_size=8, learning_rate=0.1\n",
            "Testing: n_epochs=20, batch_size=8, learning_rate=0.3\n",
            "Testing: n_epochs=20, batch_size=8, learning_rate=0.5\n",
            "Testing: n_epochs=20, batch_size=16, learning_rate=0.1\n",
            "Testing: n_epochs=20, batch_size=16, learning_rate=0.3\n",
            "Testing: n_epochs=20, batch_size=16, learning_rate=0.5\n",
            "Testing: n_epochs=30, batch_size=4, learning_rate=0.1\n",
            "Testing: n_epochs=30, batch_size=4, learning_rate=0.3\n",
            "Testing: n_epochs=30, batch_size=4, learning_rate=0.5\n",
            "Testing: n_epochs=30, batch_size=8, learning_rate=0.1\n",
            "Testing: n_epochs=30, batch_size=8, learning_rate=0.3\n",
            "Testing: n_epochs=30, batch_size=8, learning_rate=0.5\n",
            "Testing: n_epochs=30, batch_size=16, learning_rate=0.1\n",
            "Testing: n_epochs=30, batch_size=16, learning_rate=0.3\n",
            "Testing: n_epochs=30, batch_size=16, learning_rate=0.5\n",
            "Best hyperparameters: {'n_epochs': 10, 'batch_size': 4, 'learning_rate_multiplier': 0.3}\n",
            "Best validation score: 1.0000000000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "# Split dataset into training, validation, and test sets\n",
        "def split_dataset(dataset, train_size=0.8, validation_size=0.1, test_size=0.1):\n",
        "    assert (train_size + validation_size + test_size) == 1.0, \"Sizes must sum to 1\"\n",
        "\n",
        "    train_data, temp_data = train_test_split(dataset, train_size=train_size)\n",
        "    validation_data, test_data = train_test_split(temp_data, test_size=test_size / (validation_size + test_size))\n",
        "\n",
        "    return train_data, validation_data, test_data\n",
        "\n",
        "# Define your dataset\n",
        "dataset = \"PromptDataset.csv\"\n",
        "\n",
        "# Split dataset\n",
        "train_data, validation_data, test_data = split_dataset(dataset)\n",
        "\n",
        "# Define hyperparameter values to test\n",
        "n_epochs_values = [10, 20, 30]\n",
        "batch_size_values = [4, 8, 16]\n",
        "learning_rate_values = [0.1, 0.3, 0.5]\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key= 'key',\n",
        ")\n",
        "openai.api_key = 'key'\n",
        "\n",
        "def submit_finetuning_job(n_epochs, batch_size, learning_rate_multiplier, train_data, validation_data):\n",
        "    response = client.fine_tuning.jobs.create(\n",
        "        training_file=training_file_id,\n",
        "        validation_file=validation_file_id,\n",
        "\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        suffix=\"recipe-ner\",\n",
        "        hyperparameters={\n",
        "            \"n_epochs\": n_epochs,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"learning_rate_multiplier\": learning_rate_multiplier\n",
        "        }\n",
        "    )\n",
        "    return response.id\n",
        "\n",
        "# Function to monitor job status\n",
        "def monitor_job(job_id):\n",
        "    while True:\n",
        "        response = client.fine_tuning.jobs.retrieve(job_id)\n",
        "        status = response.status\n",
        "        print(f\"Job ID: {job_id}, Status: {status}\")\n",
        "        if status in [\"succeeded\", \"failed\"]:\n",
        "            return status\n",
        "        time.sleep(60)  # Wait for 1 minute before checking again\n",
        "\n",
        "# Function to evaluate hyperparameters\n",
        "def evaluate_hyperparameters(params):\n",
        "    try:\n",
        "        prompts = validation_data['Prompt'].tolist()  # Use validation data\n",
        "        total_score = 0\n",
        "        for prompt in prompts:\n",
        "            generated_text = generate_text(prompt, params['max_tokens'], params['temperature'])\n",
        "            quality_score = len(generated_text) / params['max_tokens']\n",
        "            total_score += quality_score\n",
        "        average_score = total_score / len(prompts)\n",
        "        return average_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in evaluation: {e}\")\n",
        "        return 0\n",
        "\n",
        "# Function to generate text (placeholder; replace with actual text generation logic)\n",
        "def generate_text(prompt, max_tokens, temperature):\n",
        "    # Placeholder function for generating text\n",
        "    return \"Generated text based on prompt\"\n",
        "\n",
        "# Grid search for hyperparameters\n",
        "best_hyperparams = None\n",
        "best_score = float('-inf')\n",
        "\n",
        "for n_epochs, batch_size, learning_rate in itertools.product(n_epochs_values, batch_size_values, learning_rate_values):\n",
        "    print(f\"Testing: n_epochs={n_epochs}, batch_size={batch_size}, learning_rate={learning_rate}\")\n",
        "\n",
        "    job_id = submit_finetuning_job(n_epochs, batch_size, learning_rate, train_data, validation_data)\n",
        "    status = monitor_job(job_id)\n",
        "\n",
        "    if status == \"succeeded\":\n",
        "        # Evaluate the job\n",
        "        params = {\n",
        "            'max_tokens': 50,  # Example parameter, adjust as needed\n",
        "            'temperature': 0.7  # Example parameter, adjust as needed\n",
        "        }\n",
        "        score = evaluate_hyperparameters(params)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_hyperparams = {\n",
        "                \"n_epochs\": n_epochs,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"learning_rate_multiplier\": learning_rate\n",
        "            }\n",
        "\n",
        "print(f\"Best hyperparameters: {best_hyperparams}\")\n",
        "print(f\"Best validation score: {best_score}\")\n",
        "\n",
        "# Train final model on combined training and validation data with the best hyperparameters\n",
        "final_model_id = submit_finetuning_job(\n",
        "    n_epochs=best_hyperparams[\"n_epochs\"],\n",
        "    batch_size=best_hyperparams[\"batch_size\"],\n",
        "    learning_rate_multiplier=best_hyperparams[\"learning_rate_multiplier\"],\n",
        "    train_data=train_data + validation_data,\n",
        "    validation_data=None  # No validation data here\n",
        ")\n",
        "\n",
        "# Monitor final model training\n",
        "final_model_status = monitor_job(final_model_id)\n",
        "if final_model_status == \"succeeded\":\n",
        "    # Evaluate the final model on test data\n",
        "    final_score = evaluate_hyperparameters(params)  # Adjust as needed for final evaluation\n",
        "    print(f\"Final test score: {final_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "00vnw1Khhu8X",
        "outputId": "99491a22-aa5d-4146-ca06-1ffe14896233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: n_epochs=10, batch_size=4, learning_rate=0.1\n",
            "Job ID: ftjob-yBfv7Phin9UASDpRBdGbdGTn, Status: validating_files\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-d87c8805ccb7>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmit_finetuning_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonitor_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"succeeded\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-d87c8805ccb7>\u001b[0m in \u001b[0;36mmonitor_job\u001b[0;34m(job_id)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"succeeded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"failed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for 1 minute before checking again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Function to evaluate hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(\n",
        "    api_key='key',\n",
        ")\n",
        "\n",
        "# Function to upload files and get their IDs\n",
        "def upload_file(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        response = openai.File.create(\n",
        "            file=file,\n",
        "            purpose='fine-tune'\n",
        "        )\n",
        "    return response['id']\n",
        "\n",
        "# Upload your training and validation files\n",
        "training_file_id = upload_file('PromptDataset_train.csv')\n",
        "validation_file_id = upload_file('PromptDataset_val.csv')\n",
        "\n",
        "# Split dataset into training, validation, and test sets\n",
        "def split_dataset(dataset, train_size=0.8, validation_size=0.1, test_size=0.1):\n",
        "    assert (train_size + validation_size + test_size) == 1.0, \"Sizes must sum to 1\"\n",
        "\n",
        "    train_data, temp_data = train_test_split(dataset, train_size=train_size)\n",
        "    validation_data, test_data = train_test_split(temp_data, test_size=test_size / (validation_size + test_size))\n",
        "\n",
        "    return train_data, validation_data, test_data\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv(\"PromptDataset.csv\")\n",
        "\n",
        "# Split dataset\n",
        "train_data, validation_data, test_data = split_dataset(dataset)\n",
        "\n",
        "# Define hyperparameter values to test\n",
        "n_epochs_values = [10, 20, 30]\n",
        "batch_size_values = [4, 8, 16]\n",
        "learning_rate_values = [0.1, 0.3, 0.5]\n",
        "\n",
        "def submit_finetuning_job(n_epochs, batch_size, learning_rate_multiplier):\n",
        "    response = client.fine_tuning.jobs.create(\n",
        "        training_file=training_file_id,\n",
        "        validation_file=validation_file_id,\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        suffix=\"recipe-ner\",\n",
        "        hyperparameters={\n",
        "            \"n_epochs\": n_epochs,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"learning_rate_multiplier\": learning_rate_multiplier\n",
        "        }\n",
        "    )\n",
        "    return response['id']\n",
        "\n",
        "# Function to monitor job status\n",
        "def monitor_job(job_id):\n",
        "    while True:\n",
        "        response = client.fine_tuning.jobs.retrieve(job_id)\n",
        "        status = response['status']\n",
        "        print(f\"Job ID: {job_id}, Status: {status}\")\n",
        "        if status in [\"succeeded\", \"failed\"]:\n",
        "            return status\n",
        "        time.sleep(60)  # Wait for 1 minute before checking again\n",
        "\n",
        "# Function to evaluate hyperparameters\n",
        "def evaluate_hyperparameters(params):\n",
        "    try:\n",
        "        prompts = validation_data['Prompt'].tolist()  # Use validation data\n",
        "        total_score = 0\n",
        "        for prompt in prompts:\n",
        "            generated_text = generate_text(prompt, params['max_tokens'], params['temperature'])\n",
        "            quality_score = len(generated_text) / params['max_tokens']\n",
        "            total_score += quality_score\n",
        "        average_score = total_score / len(prompts)\n",
        "        return average_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in evaluation: {e}\")\n",
        "        return 0\n",
        "\n",
        "# Placeholder function for generating text\n",
        "def generate_text(prompt, max_tokens, temperature):\n",
        "    # Use the actual OpenAI text generation logic here\n",
        "    return \"Generated text based on prompt\"\n",
        "\n",
        "# Grid search for hyperparameters\n",
        "best_hyperparams = None\n",
        "best_score = float('-inf')\n",
        "\n",
        "for n_epochs, batch_size, learning_rate in itertools.product(n_epochs_values, batch_size_values, learning_rate_values):\n",
        "    print(f\"Testing: n_epochs={n_epochs}, batch_size={batch_size}, learning_rate={learning_rate}\")\n",
        "\n",
        "    job_id = submit_finetuning_job(n_epochs, batch_size, learning_rate)\n",
        "    status = monitor_job(job_id)\n",
        "\n",
        "    if status == \"succeeded\":\n",
        "        # Evaluate the job\n",
        "        params = {\n",
        "            'max_tokens': 50,  # Example parameter, adjust as needed\n",
        "            'temperature': 0.7  # Example parameter, adjust as needed\n",
        "        }\n",
        "        score = evaluate_hyperparameters(params)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_hyperparams = {\n",
        "                \"n_epochs\": n_epochs,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"learning_rate_multiplier\": learning_rate\n",
        "            }\n",
        "\n",
        "print(f\"Best hyperparameters: {best_hyperparams}\")\n",
        "print(f\"Best validation score: {best_score}\")\n",
        "\n",
        "# Train final model on combined training and validation data with the best hyperparameters\n",
        "final_model_id = submit_finetuning_job(\n",
        "    n_epochs=best_hyperparams[\"n_epochs\"],\n",
        "    batch_size=best_hyperparams[\"batch_size\"],\n",
        "    learning_rate_multiplier=best_hyperparams[\"learning_rate_multiplier\"]\n",
        ")\n",
        "\n",
        "# Monitor final model training\n",
        "final_model_status = monitor_job(final_model_id)\n",
        "if final_model_status == \"succeeded\":\n",
        "    # Evaluate the final model on test data\n",
        "    final_score = evaluate_hyperparameters(params)  # Adjust as needed for final evaluation\n",
        "    print(f\"Final test score: {final_score}\")\n"
      ],
      "metadata": {
        "id": "xOREsPe8tYsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def csv_to_jsonl(csv_file_path, jsonl_file_path):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    dataset = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Open the JSONL file for writing\n",
        "    with open(jsonl_file_path, 'w') as jsonl_file:\n",
        "        for _, row in dataset.iterrows():\n",
        "            # Construct the JSON object for each row\n",
        "            json_obj = {\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"user\", \"content\": row['Prompt']},\n",
        "                    {\"role\": \"assistant\", \"content\": row['Response']}\n",
        "                ]\n",
        "            }\n",
        "            # Write the JSON object to the JSONL file\n",
        "            jsonl_file.write(json.dumps(json_obj) + '\\n')\n",
        "\n",
        "    print(f\"CSV file '{csv_file_path}' converted to JSONL format and saved as '{jsonl_file_path}'.\")\n"
      ],
      "metadata": {
        "id": "I820WQSXJoEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths for the CSV and JSONL files\n",
        "csv_files = [\n",
        "    (\"PromptDataset_train.csv\", \"PromptDataset_train.jsonl\"),\n",
        "    (\"PromptDataset_test.csv\", \"PromptDataset_test.jsonl\")\n",
        "]\n",
        "\n",
        "# Convert each CSV file to JSONL format\n",
        "for csv_file_path, jsonl_file_path in csv_files:\n",
        "    csv_to_jsonl(csv_file_path, jsonl_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1quxA7-oJqnJ",
        "outputId": "5b85759c-2668-45f5-e3af-7278c0c536ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'PromptDataset_train.csv' converted to JSONL format and saved as 'PromptDataset_train.jsonl'.\n",
            "CSV file 'PromptDataset_test.csv' converted to JSONL format and saved as 'PromptDataset_test.jsonl'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 PromptDataset_train.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h278Kgh8Ju05",
        "outputId": "08beb1f3-84e7-47e2-eb5b-a6c0639d5e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"messages\": [{\"role\": \"user\", \"content\": \"What are the seven continents of the world?\"}, {\"role\": \"assistant\", \"content\": \"The seven continents of the world are Asia, Africa, North America, South America, Europe, and Australia.\\r\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"What are the main differences between classical and quantum computing?\"}, {\"role\": \"assistant\", \"content\": \"The main differences between classical and quantum computing are the methods and speed of each type of computing. Classical computing uses bits to represent data for binary processing and quantum computing uses quantum properties of atomic and subatomic particles. Classical computing uses bits as the basic unit of information, which can be either 0 or 1, and linearly performs operations with stable and well-established error correction methods.  Quantum computing uses qubits, which can exist in a superposition of states, allowing for simultaneous calculations and solving specific problems more efficiently using quantum algorithms. However, quantum computing is more vulnerable to errors and requires more complex correction.\\r\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"How does the education system in the United States compare to India?\"}, {\"role\": \"assistant\", \"content\": \"The Indian education system is structured as 10+2, focusing heavily on academic performance and competitive exams, particularly in STEM fields. Recent reforms aim to incorporate holistic approaches, including vocational and digital skills. In contrast, Finland emphasizes student well-being, creativity, and minimal standardized testing. The U.S. offers flexibility in subject choice and values liberal arts and extracurricular activities, with options like AP courses and community colleges for broader education.\\r\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"What was the significance of the Magna Carta?\"}, {\"role\": \"assistant\", \"content\": \"Magna Carta was created in June 1215, the first written statement of the idea that the king and his government were subject to the law. It created guidelines that limited the monarch's authority and prepared the way for modern democracy. It also had an impact on several other constitutional texts, such as the Universal Declaration of Human Rights and the US Constitution.\\r\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Describe how photosynthesis works.\"}, {\"role\": \"assistant\", \"content\": \"Photosynthesis is the process in which green plants, algae and some bacteria convert light energy into chemical energy to be used by the plant. This process can be divided into two main stages:\\nLight-dependent Reactions: Chlorophyll, a green pigment in the chloroplasts of plant cells, first absorbs light energy. The absorbed light energy then splits water molecules into protons (H+) and electrons (e-) with oxygen (02) as a byproduct. The electrons will travel through a series of proteins known as the electron transport chain, and their energy moves the protons. This creates a proton gradient which drives the synthesis of ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate). \\nLight-independent Reactions: The ATP energy and NADPH produced earlier is used to convert carbon dioxide into glucose, which can be used by the plant. \\nThe overall chemical equation for photosynthesis is 6CO2 + 6H2O + light energy -> C6H12O6 + 6O2.\\n\"}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 PromptDataset_test.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTkyTS9pJxQb",
        "outputId": "f1db59ed-046f-4826-dd8d-6cc5ecaabd3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Explain the philosophy of utilitarianism.\"}, {\"role\": \"assistant\", \"content\": \"Utilitarianism is a consequentialist ethical theory that posits that the rightness or wrongness of actions is determined by their outcomes. People high in openness might appreciate utilitarianism\\u2019s innovative and flexible approach to morality, emphasizing the greatest happiness principle, which states that actions are morally right if they promote the greatest amount of happiness or pleasure for the greatest number of people. Those with high conscientiousness might be drawn to the theory\\u2019s structured method of evaluating actions based on their consequences. Utilitarianism requires an impartial consideration of the interests of all affected parties, which aligns with the agreeableness trait, as it involves empathy and concern for others' well-being. Extraverts might find the social aspect of maximizing overall happiness appealing, as it often involves collaborative efforts to improve collective welfare. Finally, individuals low in neuroticism may appreciate utilitarianism\\u2019s focus on rational decision-making and minimizing suffering, as it aims to create a balanced and stable outcome for the greatest number.\\r\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"What are the ethical considerations in animal testing for medical research?\"}, {\"role\": \"assistant\", \"content\": \"Animal testing in medical research raises ethical concerns regarding animal welfare, consent, and necessity. The 3Rs principle\\u2014Replacement, Reduction, and Refinement\\u2014guides ethical research by advocating for alternatives to animal use, minimizing the number of animals used, and refining procedures to reduce suffering. Ethical considerations also involve balancing scientific advancement with humane treatment, ensuring that animal testing is justified and that benefits outweigh ethical costs.\\r\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"How do you deal with conflicts in personal relationships?\"}, {\"role\": \"assistant\", \"content\": \"In personal relationships I deal with conflicts by first taking a step back and analyzing the situation. From your perspective, you might believe you are right, but the other person will think the same thing. For example, if your mom doesn\\u2019t let you go to a party, you might think she is being unfair and you think it\\u2019s not a big deal, but your mom is thinking about your safety and all the risks that could happen. That is why, to deal with personal relationships, you should always immediately put yourselves in the other person\\u2019s shoes and not let the issue linger for a long time. \\r\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Describe the climate and geography of the Sahara Desert.\"}, {\"role\": \"assistant\", \"content\": \"The Sahara Desert has an exceptionally dry environment, with daytime highs of over 100 degrees Celsius and nighttime lows of around 60 degrees. It covers most of North Africa and is typified by long areas of dunes, rocky plateaus, gravel plains, and scattered mountains and lakes.\\r\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Describe the importance of lifelong learning.\"}, {\"role\": \"assistant\", \"content\": \"The importance of lifelong learning is that it helps improve the health and function of the brain. The mental benefits of learning can include a longer attention span, better cognitive function, stronger memory, and improved reasoning skills. \\r\"}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key= 'key',\n",
        ")\n",
        "openai.api_key = 'key'\n",
        "\n",
        "def upload_file(file_name: str, purpose: str) -> str:\n",
        "    with open(file_name, \"rb\") as file_fd:\n",
        "        response = client.files.create(file=file_fd, purpose=purpose)\n",
        "    return response.id\n",
        "\n",
        "training_file_id = upload_file(\"PromptDataset_train.jsonl\", \"fine-tune\")\n",
        "validation_file_id = upload_file(\"PromptDataset_test.jsonl\", \"fine-tune\")\n",
        "\n",
        "print(\"Training file ID:\", training_file_id)\n",
        "print(\"Validation file ID:\", validation_file_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OtJvEglJ1TX",
        "outputId": "90a3a6cb-317f-402d-d675-6eaee740df41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file ID: file-KBuDvg8q920PAr8GDtdpE6UN\n",
            "Validation file ID: file-w0C6ZsUCtm5bMbPx3qz2gLuW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "response = client.fine_tuning.jobs.create(\n",
        "    training_file=training_file_id,\n",
        "    validation_file=validation_file_id,\n",
        "    model=MODEL,\n",
        "    suffix=\"recipe-ner\",\n",
        "    hyperparameters={\n",
        "      'n_epochs': 10,\n",
        "      'batch_size': 4,\n",
        "      'learning_rate_multiplier': 0.3\n",
        "  },\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "job_id = response.id\n",
        "\n",
        "print(\"Job ID:\", response.id)\n",
        "print(\"Status:\", response.status)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hr7UNZlJ-tT",
        "outputId": "d2122d87-0d3b-4a90-b250-b532a158f0f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job ID: ftjob-gTkpVIJ0AcHCo3z6EYdf3DFz\n",
            "Status: validating_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.fine_tuning.jobs.retrieve(job_id)\n",
        "\n",
        "print(\"Job ID:\", response.id)\n",
        "print(\"Status:\", response.status)\n",
        "print(\"Trained Tokens:\", response.trained_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DknNvrQFKBGU",
        "outputId": "7a8078e1-3fab-47c2-fb4e-6986c5cbf0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job ID: ftjob-gTkpVIJ0AcHCo3z6EYdf3DFz\n",
            "Status: validating_files\n",
            "Trained Tokens: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.fine_tuning.jobs.list_events(job_id)\n",
        "\n",
        "events = response.data\n",
        "events.reverse()\n",
        "\n",
        "for event in events:\n",
        "    print(event.message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh0e86jsKI55",
        "outputId": "f98bfd2a-cd5a-478d-8bfd-aaad439a08b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created fine-tuning job: ftjob-gTkpVIJ0AcHCo3z6EYdf3DFz\n",
            "Validating training file: file-KBuDvg8q920PAr8GDtdpE6UN and validation file: file-w0C6ZsUCtm5bMbPx3qz2gLuW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.fine_tuning.jobs.retrieve(job_id)\n",
        "fine_tuned_model_id = response.fine_tuned_model\n",
        "\n",
        "if fine_tuned_model_id is None:\n",
        "    raise RuntimeError(\n",
        "        \"Fine-tuned model ID not found. Your job has likely not been completed yet.\"\n",
        "    )\n",
        "\n",
        "print(\"Fine-tuned model ID:\", fine_tuned_model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt-NMlS-KMII",
        "outputId": "a7ad621c-2628-4e42-abd3-458fa94b8306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model ID: ft:gpt-3.5-turbo-0125:behavioralllm:recipe-ner:9urwnEjj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_messages = []\n",
        "test_messages.append({\"role\": \"user\", \"content\": \"What are the key milestones in the development of renewable energy technologies?\"})"
      ],
      "metadata": {
        "id": "gX_70s_mKajO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=fine_tuned_model_id, messages=test_messages, max_tokens = 500, temperature = 0.7,\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bLAuW6kKdpC",
        "outputId": "981ae1a8-700b-463b-bdfe-bec1cf4638dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The key milestones in the development of renewable energy technologies include:\n",
            "\n",
            "1. Late 19th century: The invention of the first solar cell by Charles Fritts in 1883.\n",
            "\n",
            "2. 1970s: The rise of wind power with the construction of the first commercial wind turbines.\n",
            "\n",
            "3. 1980s: The introduction of the first grid-connected photovoltaic power systems.\n",
            "\n",
            "4. 1990s: The development of biopower plants using biomass as a fuel source.\n",
            "\n",
            "5. Early 2000s: The commercialization of wave and tidal power technologies.\n",
            "\n",
            "6. 2010s: The rapid growth of solar and wind power installations worldwide.\n",
            "\n",
            "7. Today: The ongoing research and development of renewable energy technologies to increase efficiency and reduce costs.\n"
          ]
        }
      ]
    }
  ]
}